# AIbemyEYE

Stage 0 â€” ì „ì œ/ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ (í•„ìˆ˜)

ëª©í‘œ: ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ìž…ë ¥ í˜•íƒœÂ·ì¦ê°•Â·ë©”íƒ€ë°ì´í„° ê·œê²©ì„ ì •ì˜í•œë‹¤.

ì›ì‹œ í¬ë§· ê·œê²©

í•œ ì„¼ì„œ í•­ëª©: {id, raw_rate, raw_array (numpy), meta: {type? bit_start? pair?}, time_start(optional)}

íŒŒì¼/ìŠ¤í†±ì‹œìŠ¤í…œ

í° ë°ì´í„°ëŠ” per-scene íŒŒì¼(ì˜ˆ: 10~60s ë‹¨ìœ„), ì••ì¶•ì€ zarr/hdf5 ê¶Œìž¥.

Preproc

ì •ê·œí™”: ê° ì„¼ì„œë³„ robust z-score (x - median)/MAD ë˜ëŠ” min-max(0..1) â€” rateë³„ë¡œ ì°¨ë“± ì ìš©.

Missing handling: ì„ í˜•ë³´ê°„/forward fill/flag.

Augment

Contrastiveìš©: jitter, scale, mask, cutout, bitflip(for bitfields), reorder windows (for permutation invariance experiments).

Metadata

raw32 ë³´ê´€ ë“± 2word ê´€ë ¨ ì •ë³´ëŠ” metaì— ë³´ê´€.

Stage 1 â€” Pre-classifier (data type ì‹ë³„)

ëª©ì : ê° ì„¼ì„œ(ë˜ëŠ” ì„¼ì„œ í–‰ë ¬)ë¥¼ ë³´ê³  type âˆˆ {1word, 2word, complex_bits} ë¥¼ ì˜ˆì¸¡. ë˜í•œ 2wordë¼ë©´ pair candidate(LSB/MSB) ê°€ëŠ¥ì„±ì„ ì œê³µ.

ì ‘ê·¼ (ì‹¤ë¬´ ê¶Œìž¥)

Per-rate pre-classifier: ê°™ì€ raw_rate ê·¸ë£¹ë¼ë¦¬ ë³„ë„ pre-classifier(ë¹ ë¥¸ inference).

ëª¨ë¸: 1D-CNN â†’ Attention pooling â†’ MLP classifier

ì´ìœ : CNNì€ variable length í—ˆìš©, íŒŒë¼ë¯¸í„° ì ìŒ, ë¹ ë¦„.

ìž…ë ¥: single-sensor time series (variable T)

ì¶œë ¥:

node_type_logits (3-way softmax)

pair_score_vector (N-length if entire scene provided) â€” optional: if you feed whole scene, else produce embeddings for candidate pairing stage

í•™ìŠµ

ë°ì´í„°: ì‹œë®¬ë ˆì´í„°ë¡œ ëŒ€ê·œëª¨ synthetic dataset ìƒì„± (ë‹¤ì–‘í•œ bit/word arrangements, msb/lsb ìœ„ì¹˜, random scattering).

loss: CrossEntropy(node_type) + (optional) contrastive loss for embedding separability

hparams (ê¶Œìž¥):

emb_dim = 128, lr = 1e-3, batch_size = 256 (sensor-level), epochs = 50

augment each sample twice for contrastive pretrain if desired, temp=0.1

Output ì¸í„°íŽ˜ì´ìŠ¤
preclassifier.predict(sensor_tensor) -> {
  'type_probs': [p1,p2,p3],
  'emb': vector128  # optional
}

Stage 2 â€” íƒ€ìž…ë³„ ì¸ì½”ë” + Header/Body í†µí•©

ëª©ì : ê° rate/typeì— ìµœì í™”ëœ ì¸ì½”ë”ë¡œ íŠ¹ì§•ì„ ë½‘ì•„ ê³µí†µ dimensionìœ¼ë¡œ íˆ¬ì˜(í—¤ë”), ì´í›„ ì‹œê°„Â·ì„¼ì„œ ê°„ ìƒí˜¸ìž‘ìš©(ë°”ë””)ìœ¼ë¡œ ì´ìƒì  íŒë‹¨.

2.1 Type-specific encoders (Header)

êµ¬ì¡°:

For high-rate (2kHz): TCN / 1D-CNN stack (dilated) â†’ multi-scale features â†’ temporal-compressed representation (L_bins Ã— D) + pooled vector v (D).

For mid/low-rate: ResNet1D / small Transformer â†’ same outputs.

For complex bits: bit-aware encoder:

Parse 16-bit word vector per time step into bit-channels (16 dims) â†’ small conv over time + embedding â†’ also detect local patterns such as toggles, counters.

Output:

v_i (sensor-level vector, dim D) â€” for GNN/ML fusion input

T_feat_i (temporal compressed features shape LÃ—D) â€” for overflow/time-event prediction & framewise scoring

2.2 Header Network (Projection)

All encoders project to a shared D (e.g. 128) via small MLP: proj(v) -> z.

This allows heterogeneous encoders to feed a common fusion module.

2.3 Body Network (Fusion)

Two main choices:

A) Cross-Attention Transformer (recommended)

Input: sequence of node-temporal-embeddings â€” we can represent each sensor as sequence of L tokens (temporal compressed).

Use a multi-modal cross-attention:

Per-sensor temporal tokens attend within-sensor (local) then cross-sensor.

Design: per-sensor encoder outputs LÃ—D; create tokens sensor_id + time_bin and apply Transformer layers with sparse attention (local + cross-sensor top-k).

Output: per-time-bin anomaly scores per sensor (or aggregated per global time bin).

B) GNN over sensor-nodes + temporal tokens

Build graph nodes = sensors; edges built from similarity/physical adjacency; node features = pooled + aggregated temporal summaries.

Use message passing â†’ update node state and optionally decode temporal predictions using the temporal compressed features and pairwise attention.

2.4 Output(s)

framewise anomaly score: score_{sensor, bin} (continuous)

binary decision: after EVT thresholding or percentile thresholding â†’ normal/abnormal

pairing outputs: pair likelihood matrix (for 2word detection)

bitmask predictions: for bits nodes (optional)

Losses (unsupervised / weakly-supervised)

Because labels mostly absent, use self-supervised + weak labels + EVT:

Self-supervised representation losses (pretrain)

Contrastive (NT-Xent) on v or on temporal tokens (positive = augmented views of same sensor window, negatives = other sensors/windows).

CPC (Contrastive Predictive Coding): predict future compressed token given past.

Masked reconstruction for bits (predict masked bits from context).

Reconstruction loss (optional AE head)

Reconstruct temporal compressed features, use MSE. Helps modeling typical patterns.

Temporal scoring loss (semi-supervised / pseudo)

If you have small labeled anomalies (or human in loop), supervise with BCE/CE on frame level.

Pair detection and overflow losses (for 2word)

Pair contrastive: bring MSB/LSB embeddings close (if pair known from meta or synthetic).

Overflow event detection: per-pair time-bin BCE on detected rollovers (if raw32 available in synthetic training).

Regularization

Cosine-norm constraints, KL-diversity for experts in MoE, dropout.

Loss composition example (training head & body)

L = Î»_c * L_contrastive + Î»_rec * L_recon
    + Î»_pair * L_pair + Î»_overflow * L_overflow_bins


Initial recommended Î» values:

Î»_c = 1.0, Î»_rec = 0.5, Î»_pair = 0.5, Î»_overflow = 1.0 (tune per dataset)

Stage 2 Practical Training Recipe

Pretrain encoders (self-supervised):

Per-rate pretrain using contrastive/CPC for 50â€“200 epochs.

Augmentations: jitter/mask/bitflip/time-warp

Freeze header â†’ train fusion (body) with contrastive + reconstruction:

Optionally initialize fusion with small LR on encoder params.

If small labeled set available:

Finetune with framewise BCE on anomaly labels (weak-supervision).

Estimate anomaly score distribution on validation normal data:

Collect validation normal set (or most of data assumed normal) â†’ compute scores â†’ fit EVT (Generalized Pareto Distribution) to upper tail of scores or use percentile (e.g. 99.5%) as initial threshold.

Stage 3 â€” Thresholding, Human-in-the-loop, Continual Finetune
EVT thresholding (recommended)

For anomaly scores s_t, estimate tail behavior:

Choose high threshold u (e.g., 95th percentile of s_t on validation normal set)

Collect exceedances y = s_t - u and fit GPD to y

Choose target false alarm rate Î± (e.g., 1e-3/day) â†’ invert GPD to get decision threshold s*

Practical simplification: if no EVT impl, use percentile thresholds (99.7%).

Human-in-the-loop

Show top-k highest scoring windows to an operator for labeling (active learning).

Two feedback styles:

Binary correction: operator marks window normal/abnormal â†’ add as labeled examples for finetune.

Weak region marking: operator marks long intervals abnormal â†’ produce weak labels for MIL (Multiple Instance Learning).

After accumulating N_feedback (e.g., 200 samples), perform finetune:

Small LR, weighted BCE with positive class upsampling, early stopping.

Continual learning / drift adaptation

Keep an online buffer of recent features/scores.

Drift detection:

Monitor embedding distribution shift via KL divergence or MMD between current window and baseline.

If drift detected, trigger unsupervised adaptation: fine-tune encoders on recent data with self-supervised objectives (contrastive + replay).

Use regularized finetune: LR low (1e-5), batchnorm momentum reset, freeze lower layers.

Stage 4 â€” Evaluation & Metrics

Because supervised labels are scarce, use a mixture:

A. If small labeled eval set exists:

Frame-level: Precision/Recall/F1, AUROC (score vs label), AUPR

Event-level: segment-level IoU (predicted abnormal segments vs GT segments)

Pair detection (for 2word): precision/recall/F1 on pair edges

Bitmask IoU for bits nodes

B. Unsupervised diagnostics (no labels)

Reconstruction error distribution on held-out normal set

Embedding cluster stability (k-means inertia over time)

Alarm rate vs. operator time (false alarm rate) â€” major production metric

C. Drift & robustness

Monitor score quantiles (median, 95th, 99.9th) over sliding windows â†’ alert if shifting significantly.


```mermaid
flowchart TD
  A[Raw multi-rate sensors] --> B[Preprocessing & augmentation]
  B --> C[Pre-classifier per-rate] 
  C --> D1[Type-specific Encoder A '2kHz]
  C --> D2[Type-specific Encoder B '200Hz']
  C --> D3[Type-specific Encoder C '50Hz']
  D1 --> E[Header projection 'shared dim']
  D2 --> E
  D3 --> E
  E --> F[Temporal Fusion Body 'cross-attention / GNN']
  F --> G[Anomaly Scorer per time-frame]
  G --> H[EVT thresholding & alerts]
  H --> I[Human-in-the-loop labeling / feedback]
  I --> J[Continual fine-tune / drift detection]


```

íŒŒì¼ êµ¬ì¡°
``` bash
project/
â”œâ”€ data/
â”‚  â”œâ”€ generator.py            # realistic generator for sim data
â”‚  â”œâ”€ dataset.py              # SceneDataset, sensor loaders
â”œâ”€ models/
â”‚  â”œâ”€ preclassifier.py        # per-rate pre-classifier model
â”‚  â”œâ”€ encoders.py             # rate-specific encoders
â”‚  â”œâ”€ fusion.py               # cross-attention/GNN fusion
â”‚  â”œâ”€ heads.py                # anomaly head, overflow head, pair head
â”œâ”€ train/
â”‚  â”œâ”€ pretrain.py             # contrastive pretrain scripts
â”‚  â”œâ”€ finetune.py             # fusion training with loss composition
â”‚  â”œâ”€ evaluate.py             # metrics, EVT thresholding
â”œâ”€ tools/
â”‚  â”œâ”€ augment.py
â”‚  â”œâ”€ evt.py                  # EVT fit & threshold utilities (GPD)
â”‚  â”œâ”€ viz.py
â””â”€ experiments/
   â””â”€ config.yaml

```



Soft â†” Hard gating ì „í™˜ ì „ëžµ í¬í•¨ MoE multi-modal pipelineì„ Mermaid flowë¡œ ì‹œê°í™”

**heterogeneous ì„¼ì„œ ìž…ë ¥, gating network, expert encoders, soft/hard gating ì„ íƒ, fusion, shared embedding, downstream task, EVT thresholdê¹Œì§€ í¬í•¨ë©ë‹ˆë‹¤.**

```mermaid
flowchart TD
    %% ================= Inputs =================
    subgraph Inputs["ðŸŸ¦ Multi-Modal Sensor Inputs"]
        X1["X1: Binary / 1bit"]
        X2["X2: 2-word / 16bit"]
        X3["X3: Float / Int"]
        X4["X4: Multi-bit (3~15 bits)"]
    end

    %% ================= Initial Embedding =================
    subgraph InitEmbed["ðŸŸ¨ Initial Embedding / Preprocessing"]
        PRE1["Binary â†’ bitwise linear â†’ hidden_dim"]
        PRE2["2-word â†’ optional bit embedding â†’ linear â†’ hidden_dim"]
        PRE3["Float/Int â†’ linear â†’ hidden_dim"]
        PRE4["Multi-bit â†’ linear â†’ hidden_dim"]
    end

    X1 --> PRE1
    X2 --> PRE2
    X3 --> PRE3
    X4 --> PRE4

    %% ================= Gating Network =================
    subgraph Gating["ðŸŸ© Gating Network"]
        GATE["MLP â†’ softmax â†’ gating_prob (soft)"]
        SOFT["Soft Gating â†’ weighted sum of all Expert outputs"]
        HARD["Hard Gating â†’ argmax / top-k selection"]
        SWITCH["Soft â†” Hard Switch (training phase dependent)"]
    end

    PRE1 --> GATE
    PRE2 --> GATE
    PRE3 --> GATE
    PRE4 --> GATE
    GATE --> SWITCH
    SWITCH --> SOFT
    SWITCH --> HARD

    %% ================= Expert Encoders =================
    subgraph Experts["ðŸŸ§ Expert Encoders (MoE)"]
        E1["Expert 1: Binary-focused"]
        E2["Expert 2: Float/Int-focused"]
        E3["Expert 3: Multi-bit / High-cardinality"]
    end

    SOFT --> E1
    SOFT --> E2
    SOFT --> E3

    HARD --> E1
    HARD --> E2
    HARD --> E3

    %% ================= Fusion Layer =================
    subgraph Fusion["ðŸŸ¨ Fusion Layer / Shared Embedding"]
        CONCAT["Concat / Attention / Weighted sum"]
        Z_shared["Shared embedding z_t"]
    end

    E1 --> CONCAT
    E2 --> CONCAT
    E3 --> CONCAT
    CONCAT --> Z_shared

    %% ================= Downstream =================
    subgraph Downstream["ðŸŸª Downstream Tasks"]
        PRETRAIN["Self-Supervised: CPC / Contrastive / Recon Loss"]
        FINETUNE["MIL / Weak Label Fine-tune"]
        HUMAN["Human Feedback / Pseudo-label update"]
        CONTINUAL["Continual Fine-tune / Drift adaptation"]
        EVT["EVT / Percentile Threshold"]
        LABEL["Normal / Abnormal Label"]
    end

    Z_shared --> PRETRAIN --> FINETUNE --> HUMAN --> CONTINUAL --> EVT --> LABEL

    %% ================= Legend / Notes =================
    classDef inputs fill:#d0ebff,stroke:#000,stroke-width:1px;
    classDef preprocessing fill:#fff3bf,stroke:#000,stroke-width:1px;
    classDef gating fill:#c3f0ca,stroke:#000,stroke-width:1px;
    classDef experts fill:#ffd6d6,stroke:#000,stroke-width:1px;
    classDef fusion fill:#fff3bf,stroke:#000,stroke-width:1px;
    classDef downstream fill:#e0c3ff,stroke:#000,stroke-width:1px;

    class Inputs inputs;
    class InitEmbed preprocessing;
    class Gating gating;
    class Experts experts;
    class Fusion fusion;
    class Downstream downstream;

```


ì‚¬ìš© ì„¤ëª… (ê°„ë‹¨)

generate_multimodal_data_advanced(...) í˜¸ì¶œë¡œ ì„¼ì„œ ë¦¬ìŠ¤íŠ¸ì™€ (N x target_T) ì •ë ¬ëœ í–‰ë ¬ì„ ì–»ìŠµë‹ˆë‹¤.

MoEMultiSensorDatasetëŠ” PyTorch í•™ìŠµ ë£¨í‹´ì— ë°”ë¡œ ì‚¬ìš©ë  ìˆ˜ ìžˆëŠ” í˜•íƒœë¡œ ê° ì„¼ì„œì˜ ì›ì‹œ(raw)ì™€ ì •ë ¬(aligned) ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

anomaly insertëŠ” anomaly_cfg íŒŒë¼ë¯¸í„°ë¡œ ì œì–´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

use_multirate=Trueì´ë©´ ì„¼ì„œë³„ë¡œ ëžœë¤í•˜ê²Œ 50/200/2000Hzë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ì‹œ ë°ì´í„°ë¥¼ ë§Œë“¤ê³ , ë§ˆì§€ë§‰ì— ëª¨ë‘ resampleë¡œ target_T ê¸¸ì´ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.




**"main_pre_classifier.py"** êµ¬ì¡°

```mermaid
flowchart TB
    subgraph INPUT
        S[scene: list of sensors] --> RAW[raw np.array/tensor/list, raw_rate, type, meta]
    end

    subgraph ENCODERS
        RAW --> ENC[RateEncoderTemporal<br>outputs: vec + temporal features]
    end

    subgraph GRAPH
        ENC --> X[X: N x emb]
        X --> G1[SimpleGraphLayer g1]
        G1 --> H1[H1: N x emb]
        H1 --> G2[SimpleGraphLayer g2]
        G2 --> H2[H2: N x emb]
    end

    subgraph HEADS
        H2 --> NODE[node_head: N x 3]
        H2 --> BIT[bitmask_head: N x 16]
        H2 --> PAIR[pair_feat: N x N x 2*emb]
        PAIR --> EDGE[edge_head: N x N]
        PAIR --> ORDER[order_head: N x N x 2]
        ENC --> TEMP[temps: list of L x emb]
        TEMP --> OVERFLOW[overflow temporal conv per-pair<br>conv1->ReLU->conv2->ReLU->conv_out]
    end

    subgraph OUTPUT
        NODE --> OUT[node_logits: N x 3]
        EDGE --> OUT2[edge_logits: N x N]
        ORDER --> OUT3[order_logits: N x N x 2]
        BIT --> OUT4[bitmask_logits: N x 16]
        OVERFLOW --> OUT5[overflow_logits: N x N x L]
        G2 --> ATT[attention: N x N]
    end
```


**Contrastive Learning(pretrain)**

```mermaid
flowchart TB
    subgraph PRETRAIN_Contrastive
        POOL[Collect  raw, rate from all scenes] --> BATCH[Random batch]

        BATCH --> LOOP1[For each sensor]
        LOOP1 --> RAW[Convert raw to 1D tensor]
        RAW --> AUG1[Aug1]
        RAW --> AUG2[Aug2]

        AUG1 --> ENC1[Encoder rate]
        AUG2 --> ENC2[Encoder rate]

        ENC1 --> Z1[z1 batch]
        ENC2 --> Z2[z2 batch]

        Z1 --> NTX[NT-Xent loss]
        Z2 --> NTX

        NTX --> OPT[Adam update]
    end

```



**Full fine-tue Learning**

```mermaid
flowchart TB
    subgraph Finetune
        TRAIN[Train Dataset Scenes] --> LOADER[SceneDataLoader]

        LOADER --> SCENE[Scene list]

        SCENE --> GT[Build Ground Truth numpy->tensor]
        GT --> GTD[Move GT to device]

        SCENE --> MODEL[SensorStructureModel forward]
        MODEL --> OUT[Outputs dict]

        OUT --> SAN[Sanitize outputs nan->num]
        GTD --> SANGT[Sanitize GT]

        SAN --> LOSS[Compute all losses]
        SANGT --> LOSS

        LOSS --> CHECK[Check finite]
        CHECK -->|finite| BACKWARD[Backward + Clip + Optim Step]
        CHECK -->|not finite| SKIP[Skip step]

        BACKWARD --> ACC[Accumulate loss mean]

        ACC --> END1[Print epoch train loss]

        subgraph Validation
            VALDS[Val dataset] --> VALLOADER
            VALLOADER --> VSC[V scene]
            VSC --> VGT[GT build]
            VSC --> VOUT[Model forward]
            VGT --> MET[Metrics]
            VOUT --> MET
            MET --> END2[Print validation metrics]
        end
    end

```




** Module Structure - ì „ì²´ ëª¨ë¸ êµ¬ì¡° **
``` mermaid
flowchart LR
    %% INPUT
    SENS[Scene sensors list] --> RATESEL[Select encoder by raw_rate]

    %% ENCODER
    RATESEL --> ENC[RateEncoderTemporal]
    ENC --> VECS[vec per sensor]
    ENC --> TEMP[tfeat per sensor L x Emb]

    %% BUILD X
    VECS --> X[N x Emb]

    %% GRAPH STACK
    X --> G1[GraphLayer1]
    G1 --> H1[N x Emb]

    H1 --> G2[GraphLayer2]
    G2 --> H2[N x Emb]
    G2 --> ATT[attention N x N]

    %% HEADS
    H2 --> NODE[node head N x 3]
    H2 --> BIT[bitmask head N x 16]

    H2 --> PAIR[pair features N x N x 2Emb]

    PAIR --> EDGE[edge head N x N]
    PAIR --> ORDER[order head N x N x 2]

    TEMP --> OVRCONV[overflow temporal conv stack]
    OVRCONV --> OVR[overflow logits N x N x L]

    %% OUTPUT
    NODE --> O1[node_logits]
    BIT --> O2[bitmask_logits]
    EDGE --> O3[edge_logits]
    ORDER --> O4[order_logits]
    OVR --> O5[overflow_logits]
    ATT --> O6[attention_matrix]

```

** Module Structure(Simple GraphLayer) **
``` mermaid
flowchart TB
    subgraph SimpleGraphLayer
        X[Input X N x Emb] --> Q[Linear -> Q]
        X --> K[Linear -> K]
        X --> V[Linear -> V]

        Q --> MATMUL1[Q x K^T / sqrt Emb ]
        K --> MATMUL1
        MATMUL1 --> SOFT[Softmax row-wise]
        SOFT --> ATT[Attention A N x N]

        ATT --> MATMUL2[A x V]
        V --> MATMUL2

        MATMUL2 --> RES[Residual + X]
        RES --> OUT_L[Linear]
        OUT_L --> REL[ReLU]
        REL --> OUT[Output H N x Emb]
    end

```

** Module Structure(Rate Encoder Temporal) **
```mermaid

flowchart TB
    subgraph RateEncoderTemporal
        X[Input 1D time-series T] --> U1[Unsqueeze to 1x1xT]
        U1 --> C1[Conv1d 1->Cc k7]
        C1 --> R1[ReLU]
        R1 --> C2[Conv1d Cc->Cc k5]
        C2 --> R2[ReLU]

        %% vector branch
        R2 --> P1[AdaptiveAvgPool1d 1]
        P1 --> S1[Squeeze]
        S1 --> FC[Linear Cc->Emb]
        FC --> Vec[Vector Emb]

        %% temporal branch
        R2 --> TP[AdaptiveAvgPool1d L bins]
        TP --> PROJ[Conv1d Cc->Emb k1]
        PROJ --> TFEAT[Squeeze and Permute L x Emb]

        Vec --> OUT1(Output vec)
        TFEAT --> OUT2(Output temporal)
    end
```
