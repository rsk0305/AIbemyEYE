{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a0d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.models.preclassifier import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70be1372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu | CPU cores: 2 | RAM: ~24GB reported\n",
      "=== Contrastive pretrain encoders ===\n",
      "[Pretrain] Pool size: 1487 sensors\n",
      "[Pretrain] Epoch 1/2 avg_loss=4.618612\n",
      "[Pretrain] Epoch 2/2 avg_loss=4.248173\n",
      "=== Finetune full model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/AIbemyEYE/src/models/preclassifier.py:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "/workspaces/AIbemyEYE/src/models/preclassifier.py:368: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Finetune] Ep 1/6 train_loss=29.252389 (updated_steps=100)\n",
      "[Val] node_acc=0.663 edge_prf_mean=0.000 bit_iou=0.151 overflow_f1=0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/AIbemyEYE/src/models/preclassifier.py:368: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Finetune] Ep 2/6 train_loss=24.899688 (updated_steps=100)\n",
      "[Val] node_acc=0.728 edge_prf_mean=0.000 bit_iou=0.793 overflow_f1=0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/AIbemyEYE/src/models/preclassifier.py:368: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Finetune] Ep 3/6 train_loss=23.366980 (updated_steps=100)\n",
      "[Val] node_acc=0.768 edge_prf_mean=0.000 bit_iou=0.038 overflow_f1=0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/AIbemyEYE/src/models/preclassifier.py:368: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Finetune] Ep 4/6 train_loss=23.120517 (updated_steps=100)\n",
      "[Val] node_acc=0.922 edge_prf_mean=0.000 bit_iou=0.253 overflow_f1=0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/AIbemyEYE/src/models/preclassifier.py:368: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Finetune] Ep 5/6 train_loss=23.115465 (updated_steps=100)\n",
      "[Val] node_acc=0.919 edge_prf_mean=0.000 bit_iou=0.552 overflow_f1=0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/AIbemyEYE/src/models/preclassifier.py:368: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Finetune] Ep 6/6 train_loss=22.992995 (updated_steps=100)\n",
      "[Val] node_acc=0.909 edge_prf_mean=0.054 bit_iou=0.085 overflow_f1=0.045\n",
      "[Checkpoint] saved checkpoints/model_ckpt.pth\n",
      "=== Evaluation on training set ===\n",
      "[Eval] scenes=100 node_acc=0.9043 edge_f1=0.0972 bit_iou=0.0908 overflow_f1=0.0465\n",
      "=== Evaluation on validation set ===\n",
      "[Eval] scenes=20 node_acc=0.9091 edge_f1=0.0542 bit_iou=0.0848 overflow_f1=0.0446\n",
      "=== Reload checkpoint and continue training (compare) ===\n",
      "[Checkpoint] loaded checkpoints/model_ckpt.pth (epoch=6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/AIbemyEYE/src/models/preclassifier.py:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "/workspaces/AIbemyEYE/src/models/preclassifier.py:368: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Finetune] Ep 1/3 train_loss=22.973397 (updated_steps=100)\n",
      "[Val] node_acc=0.914 edge_prf_mean=0.000 bit_iou=0.553 overflow_f1=0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/AIbemyEYE/src/models/preclassifier.py:368: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Finetune] Ep 2/3 train_loss=22.791952 (updated_steps=100)\n",
      "[Val] node_acc=0.943 edge_prf_mean=0.000 bit_iou=0.434 overflow_f1=0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/AIbemyEYE/src/models/preclassifier.py:368: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Finetune] Ep 3/3 train_loss=22.739457 (updated_steps=100)\n",
      "[Val] node_acc=0.949 edge_prf_mean=0.019 bit_iou=0.542 overflow_f1=0.045\n",
      "=== Post-resume evaluation on validation set ===\n",
      "[Eval] scenes=20 node_acc=0.9492 edge_f1=0.0194 bit_iou=0.5416 overflow_f1=0.0446\n",
      "Summary: val before resume: {'node_acc': 0.9091369047619047, 'edge_f1': 0.05416666666666666, 'bit_iou': 0.08482993197278912, 'overflow_f1': 0.04456415822105392}  val after resume: {'node_acc': 0.9491666666666667, 'edge_f1': 0.01944444444444444, 'bit_iou': 0.5415618359814788, 'overflow_f1': 0.04456415822105392}\n",
      "\n",
      "================================================================================\n",
      "=== PRE-CLASSIFIER: SENSOR TYPE CLASSIFICATION ===\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Demo / main\n",
    "# -------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device} | CPU cores: {multiprocessing.cpu_count()} | RAM: ~24GB reported\")\n",
    "# generate scenes\n",
    "scenes_train = [ example_scene_generator(num_sensors=12, duration_sec=1.0, seed=i) for i in range(100) ]\n",
    "scenes_val = [ example_scene_generator(num_sensors=12, duration_sec=1.0, seed=100+i) for i in range(20) ]\n",
    "# Pretrain: collect encoders dict\n",
    "model = SensorStructureModel(rates=[200], emb_dim=128, time_bins=32, device=device)\n",
    "\n",
    "# Pretrain encoders\n",
    "print(\"=== Contrastive pretrain encoders ===\")\n",
    "encoders = model.encoders\n",
    "pretrain_contrastive(encoders, scenes_train, device=device, epochs=2, batch_size=128, lr=1e-3)\n",
    "\n",
    "num_epochs = 6\n",
    "# Fine-tune full model\n",
    "train_ds = SceneDataset(scenes_train)\n",
    "val_ds = SceneDataset(scenes_val)\n",
    "print(\"=== Finetune full model ===\")\n",
    "finetune_full(model, train_ds, val_ds, device=device, epochs=num_epochs, lr=1e-3)\n",
    "# Save checkpoint\n",
    "ckpt_path = \"checkpoints/model_ckpt.pth\"\n",
    "save_checkpoint(ckpt_path, model, optimizer=None, epoch=num_epochs, extra={'rates': model.rates})\n",
    "# Evaluate and print\n",
    "print(\"=== Evaluation on training set ===\")\n",
    "res_train = evaluate_model(model, train_ds, device=device)\n",
    "print(\"=== Evaluation on validation set ===\")\n",
    "res_val = evaluate_model(model, val_ds, device=device)\n",
    "# Load into fresh model and continue training for comparison\n",
    "print(\"=== Reload checkpoint and continue training (compare) ===\")\n",
    "model2 = SensorStructureModel(rates=[200], emb_dim=128, time_bins=32, device=device)\n",
    "load_checkpoint(ckpt_path, model2, map_location=device)\n",
    "# continue training few more epochs\n",
    "finetune_full(model2, train_ds, val_ds, device=device, epochs=3, lr=5e-4)\n",
    "print(\"=== Post-resume evaluation on validation set ===\")\n",
    "res_val2 = evaluate_model(model2, val_ds, device=device)\n",
    "print(\"Summary: val before resume:\", res_val, \" val after resume:\", res_val2)\n",
    "\n",
    "# ===== PRE-CLASSIFIER INTEGRATION =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=== PRE-CLASSIFIER: SENSOR TYPE CLASSIFICATION ===\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52607022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pre-classifier] Initializing...\n",
      "\n",
      "[Pre-classifier] Training on train set...\n",
      "[Pre-classifier] Training on 1487 samples\n",
      "[Pre-classifier] Epoch 1/10 loss=33.5838 acc=0.4607\n",
      "[Pre-classifier] Epoch 2/10 loss=6.0001 acc=0.4499\n",
      "[Pre-classifier] Epoch 3/10 loss=2.6903 acc=0.4250\n",
      "[Pre-classifier] Epoch 4/10 loss=1.8158 acc=0.4412\n",
      "[Pre-classifier] Epoch 5/10 loss=1.8108 acc=0.4196\n",
      "[Pre-classifier] Epoch 6/10 loss=1.3327 acc=0.4412\n",
      "[Pre-classifier] Epoch 7/10 loss=0.9606 acc=0.4909\n",
      "[Pre-classifier] Epoch 8/10 loss=1.0587 acc=0.5783\n",
      "[Pre-classifier] Epoch 9/10 loss=1.1752 acc=0.5837\n",
      "[Pre-classifier] Epoch 10/10 loss=0.8788 acc=0.5629\n",
      "\n",
      "[Pre-classifier] Validating on validation set...\n",
      "\n",
      "[Pre-classifier] Validation Results:\n",
      "  accuracy            : 0.6186\n",
      "  1word_precision     : 0.6990\n",
      "  1word_recall        : 1.0000\n",
      "  1word_f1            : 0.8228\n",
      "  2word_precision     : 1.0000\n",
      "  2word_recall        : 0.0098\n",
      "  2word_f1            : 0.0194\n",
      "  bits_precision      : 0.4468\n",
      "  bits_recall         : 0.8077\n",
      "  bits_f1             : 0.5753\n",
      "\n",
      "[Pre-classifier] Demo Classification on Sample Sensors:\n",
      "\n",
      "  Scene 0:\n",
      "    ✗ Sensor 0: True=2word_lsb       Pred=1word        Conf=0.850 Method=hybrid\n",
      "    ✗ Sensor 1: True=2word_msb       Pred=bits         Conf=0.950 Method=hybrid\n",
      "    ✓ Sensor 2: True=bits            Pred=bits         Conf=0.950 Method=hybrid\n",
      "    ✓ Sensor 3: True=1word           Pred=1word        Conf=0.775 Method=hybrid\n",
      "    ✗ Sensor 4: True=2word_lsb       Pred=1word        Conf=0.758 Method=hybrid\n",
      "\n",
      "  Scene 1:\n",
      "    ✗ Sensor 0: True=2word_lsb       Pred=1word        Conf=0.850 Method=hybrid\n",
      "    ✗ Sensor 1: True=2word_msb       Pred=bits         Conf=0.950 Method=hybrid\n",
      "    ✗ Sensor 2: True=2word_lsb       Pred=1word        Conf=0.850 Method=hybrid\n",
      "    ✗ Sensor 3: True=2word_msb       Pred=bits         Conf=0.950 Method=hybrid\n",
      "    ✓ Sensor 4: True=1word           Pred=1word        Conf=0.772 Method=hybrid\n",
      "\n",
      "[Pre-classifier] Summary:\n",
      "  - Accuracy: 0.6186\n",
      "  - Successfully classified 20 validation scenes\n",
      "  - Classification methods: heuristic + deep learning (hybrid)\n",
      "\n",
      "Hardware tips:\n",
      " - Use device='cuda' (Titan X) for heavy ops; enable mixed-precision (autocast used).\n",
      " - DataLoader uses pin_memory + num_workers when GPU available.\n",
      " - If memory allows, increase scene generation parallelism or batch size.\n",
      " - For CPU-bound preprocessing consider increasing num_workers or pre-saving tensors.\n",
      " - If training still slow: reduce emb_dim, reduce time_bins, or reduce dataset size for experiments.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import pre_classifier\n",
    "try:\n",
    "    from pre_classifier import SensorPreClassifier\n",
    "    \n",
    "    print(\"\\n[Pre-classifier] Initializing...\")\n",
    "    pre_classifier = SensorPreClassifier(use_deep_learning=True, device=device)\n",
    "    \n",
    "    # Convert scenes to pre_classifier format\n",
    "    def convert_scenes_format(scenes_torch):\n",
    "        converted = []\n",
    "        for scene_torch in scenes_torch:\n",
    "            scene_converted = []\n",
    "            for s in scene_torch:\n",
    "                raw_field = s['raw']\n",
    "                if isinstance(raw_field, torch.Tensor):\n",
    "                    raw_np = raw_field.detach().cpu().numpy()\n",
    "                else:\n",
    "                    raw_np = np.asarray(raw_field)\n",
    "                \n",
    "                sensor_dict = {\n",
    "                    'id': s['id'],\n",
    "                    'type': s['type'],\n",
    "                    'raw_rate': s['raw_rate'],\n",
    "                    'raw': raw_np.astype(np.float32),\n",
    "                    'meta': s.get('meta', {})\n",
    "                }\n",
    "                scene_converted.append(sensor_dict)\n",
    "            converted.append(scene_converted)\n",
    "        return converted\n",
    "    \n",
    "    # Convert train/val scenes\n",
    "    scenes_train_for_preclassifier = convert_scenes_format(scenes_train)\n",
    "    scenes_val_for_preclassifier = convert_scenes_format(scenes_val)\n",
    "    \n",
    "    # Train pre_classifier\n",
    "    print(\"\\n[Pre-classifier] Training on train set...\")\n",
    "    pre_classifier.train_on_scenes(scenes_train_for_preclassifier, device=device, \n",
    "                                    epochs=10, lr=1e-3, batch_size=64)\n",
    "    \n",
    "    # Validate pre_classifier\n",
    "    print(\"\\n[Pre-classifier] Validating on validation set...\")\n",
    "    preclassifier_metrics = pre_classifier.validate_on_scenes(\n",
    "        scenes_val_for_preclassifier, device=device\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[Pre-classifier] Validation Results:\")\n",
    "    for key, value in preclassifier_metrics.items():\n",
    "        print(f\"  {key:20s}: {value:.4f}\")\n",
    "    \n",
    "    # Demo on sample scenes\n",
    "    print(\"\\n[Pre-classifier] Demo Classification on Sample Sensors:\")\n",
    "    demo_count = 0\n",
    "    for scene_idx, scene in enumerate(scenes_val_for_preclassifier[:2]):\n",
    "        print(f\"\\n  Scene {scene_idx}:\")\n",
    "        for sensor_idx, sensor in enumerate(scene[:5]):\n",
    "            signal = sensor['raw']\n",
    "            true_type = sensor['type']\n",
    "            \n",
    "            result = pre_classifier.classify_signal(signal)\n",
    "            pred_type = result['type']\n",
    "            confidence = result['confidence']\n",
    "            \n",
    "            match = \"✓\" if pred_type in true_type or true_type in pred_type else \"✗\"\n",
    "            print(f\"    {match} Sensor {sensor_idx}: True={true_type:15s} \" +\n",
    "                    f\"Pred={pred_type:12s} Conf={confidence:.3f} Method={result['method']}\")\n",
    "            \n",
    "            demo_count += 1\n",
    "            if demo_count >= 15:\n",
    "                break\n",
    "        if demo_count >= 15:\n",
    "            break\n",
    "    \n",
    "    print(\"\\n[Pre-classifier] Summary:\")\n",
    "    print(f\"  - Accuracy: {preclassifier_metrics.get('accuracy', 0.0):.4f}\")\n",
    "    print(f\"  - Successfully classified {len(scenes_val_for_preclassifier)} validation scenes\")\n",
    "    print(f\"  - Classification methods: heuristic + deep learning (hybrid)\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"[Warning] Could not import pre_classifier: {e}\")\n",
    "    print(\"         Skipping pre-classifier integration.\")\n",
    "except Exception as e:\n",
    "    print(f\"[Error] Pre-classifier integration failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Quick hardware-aware tips\n",
    "print(\"\\nHardware tips:\")\n",
    "print(\" - Use device='cuda' (Titan X) for heavy ops; enable mixed-precision (autocast used).\")\n",
    "print(\" - DataLoader uses pin_memory + num_workers when GPU available.\")\n",
    "print(\" - If memory allows, increase scene generation parallelism or batch size.\")\n",
    "print(\" - For CPU-bound preprocessing consider increasing num_workers or pre-saving tensors.\")\n",
    "print(\" - If training still slow: reduce emb_dim, reduce time_bins, or reduce dataset size for experiments.\")\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
